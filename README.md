
# DeepFER: Facial Emotion Recognition using Deep Learning ğŸ˜„ğŸ˜¢ğŸ˜ 

> A deep learning-based system to classify facial expressions into basic human emotions using Convolutional Neural Networks (CNNs).

---

## ğŸ“Œ Overview

DeepFER (Deep Facial Emotion Recognition) is a machine learning project that classifies human emotions from facial images. This application can be used in areas like mental health analysis, smart surveillance, human-computer interaction, and more.

- ğŸ“· **Input:** Grayscale facial images (e.g., 48x48 resolution)
- ğŸ§  **Output:** Predicted emotion (Happy, Sad, Angry, Surprise, Fear, Disgust, Neutral)

---

## ğŸ” Features

- âœ… Real-time emotion detection using webcam (OpenCV)
- âœ… Trained CNN model with high accuracy on FER-2013 dataset
- âœ… Live emotion classification with UI overlay
- âœ… Visualized training history (accuracy/loss)

---

## ğŸ§  Emotions Detected

| Emotion     | Emoji     |
|-------------|-----------|
| Angry       | ğŸ˜         |
| Disgust     | ğŸ¤¢        |
| Fear        | ğŸ˜¨        |
| Happy       | ğŸ˜„        |
| Sad         | ğŸ˜¢        |
| Surprise    | ğŸ˜²        |
| Neutral     | ğŸ˜        |

---

## ğŸ—‚ï¸ Dataset

The model was trained on the **FER-2013** dataset.  
- Format: Grayscale, 48x48 px  
- Total Images: ~35000
- Classes: 7 emotion categories

Sample image:

![sample](images/sample_emotion.png)

---

## ğŸ§± Model Architecture

Built using **TensorFlow/Keras**, the CNN contains:

- Convolution layers with ReLU activation  
- MaxPooling layers  
- Dropout layers for regularization  
- Dense layers with Softmax output

```python
model = Sequential([
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')
])
```

---

## ğŸš€ How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/21ayeshashaik/deepFER.git
cd deepFER
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Train the Model

```bash
python train.py
```

### 4. Run Real-Time Emotion Detection

```bash
python live_demo.py
```

> âœ… Make sure your **webcam is enabled** and **OpenCV** is properly installed.

---

## ğŸ§ª Sample Output

![demo](images/demo.gif)

---

## ğŸ“ˆ Training Performance

- Optimizer: Adam  
- Loss Function: Categorical Crossentropy  
- Accuracy Achieved: **~%**

Training Curve:

![accuracy](images/training_accuracy.png)

---

## ğŸ› ï¸ Future Work

- ğŸ” Use transfer learning with VGGFace or MobileNet  
- âš–ï¸ Improve class balance with augmentation or weighted loss  
- ğŸ§  Explore multi-modal emotion recognition (e.g., audio + video)

---

## ğŸ™Œ Acknowledgements

- [FER-2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)  
- TensorFlow & Keras libraries  
- OpenCV for real-time video processing

---

## ğŸ“¬ Contact

**Shaik Ayesha**  
ğŸ“§ shaikayesha2107@gmail.com  
ğŸ± GitHub: [@21ayeshashaik](https://github.com/21ayeshashaik)

---

## â­ï¸ If you found this useful...

Leave a â­ï¸ and share the repo to support the project!
